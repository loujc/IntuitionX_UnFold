# AI 字幕注释产品功能
## 开发规则
1. 不要直接写代码，而是先对 spec 进行构建、规划，完成 spec 构建后严格按照 spec 驱动去做开发
2. 每次修改完之后 git commit 进行保存，commit 信息包含修改内容，方便回退。
3. 每个功能模块需要有清晰的输入输出，并且有详细注释，介绍功能和用法。
4. 函数变量均要可配置，在统一的 `config.yaml` 配置文件中进行配置，配置文件需要有详细注释。
5. 所有的脚本都要具备可扩展性、可读性，因为是一个产品，最终 AI Agent 会和前端页面交互，所以需要有后端数据库的设计，明确接口，留出接口。
6. 所有的并行数可调，并且所有与 LLM 的交互需要有重试机制，失败后保存状态自动重试。
7. **技术栈**：必须使用 OpenAI 原生 SDK，**不使用 Langchain**。
8. **时延目标**：处理时间在 20～30 秒内（Best Effort）。

## Agent 功能描述
### AI Agent
要求：
1. 使用 OpenAI 库调用 LLM，模型和各参数切换方便，上下文长度要拉满，所有的 Agent 都会调用它
3. 输出均为结构化 json 输出
4. 所有数据进数据库，不要裸输出，生成内容保留 log 与原输出。

### 1. 视频转文字稿与总结
由于视频长度可能达到 30 分钟，为了保证时效性（20-30s），必须采用 **“切片并行”** 的策略。

Input：完整视频文件
Output：带时间戳字幕 (SRT/VTT)、视频分段总结、视频类型

**核心要求与流程**：
1. **切片 (Slicing)**：先用 FFmpeg 将视频/音频物理切分为 N 个片段 (e.g. 每 5 分钟一段，切片时长可在 `config.yaml` 配置)。
2. **并行 ASR**: 使用 Whisper (Faster-Whisper/MLX-Whisper) 并行处理这些切片。
    *   **配置**: 并行 Worker 数量 (`MAX_ASR_WORKERS`) 必须在 `config.yaml` 可配。
3. **合并**: 将 ASR 结果合并为完整的带时间戳字幕。
4. **视频类型**: 根据内容判断视频类型。
    *   **配置**: 视频类型列表 (e.g., 历史、动漫、金融、课程) 必须在 `config.yaml` 中配置，支持扩展。

### 2. 提取关键词：

Input：`视频类型` + `Mode (风格模式)` + `带时间戳字幕`

Output：带时间戳字幕中的关键词 + 关键词解释 + 外链（上下文打包准备继续对话）

**要求**：
1. **模式 (Mode)**：支持 “简洁 (simple)” 和 “深度 (deep)” 模式，该参数会影响 Prompt。
    *   **Simple**: 只解释最生僻词，解释简短。
    *   **Deep**: 挖掘更多关键词，解释详细。
    *   此模式参数需透传给 LLM。
2. **视频类型 (Video Type)**：将视频类型透传给 LLM Prompt，帮助其理解上下文 (e.g. 动漫里的 "Saber" 和历史里的 "Saber" 含义不同)。
3. **并行处理**: 针对切片后的字幕并行进行挖掘，最后合并结果。
4. **外链**: 搜索关键词后给出对应资料外链 (可选，若无网络则由 LLM 生成解释)。